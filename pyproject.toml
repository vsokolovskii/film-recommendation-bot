[project]
name = "llm-chat"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "apscheduler>=3.11.0",
    "fastapi[standard]>=0.115.6",
    "openai>=1.58.1",
    "pydantic-settings>=2.7.0",
    "pydantic>=2.10.4",
    "pytelegrambotapi>=4.26.0",
    "smolagents[litellm]>=1.10.0",
    "telebot>=0.0.5",
    "tenacity>=9.0.0",
    "uvicorn>=0.34.0",
]

[dependency-groups]
dev = [
    "faker>=33.1.0",
    "httpx>=0.27.2",
    "locust>=2.32.5",
    "pytest-asyncio>=0.25.0",
    "pytest-env>=1.1.5",
    "pytest>=8.3.4",
    "ruff>=0.8.4",
]

[tool.pytest.ini_options]
testpaths = [
    "tests/test_input_length.py",
]
env = [
    "LLM_HOST=http://localhost:11434",
    "LLM_MODEL=llama3.2",
]

[tool.ruff]
src = ["src", "tests"]

[tool.ruff.lint]
select = [
    # Pyflakes
    "F",
    # Pycodestyle
    "E",
    "W",
    # isort
    "I001"
]
extend-select = [
    "I",    # isort
    "T201"  # print()
]


